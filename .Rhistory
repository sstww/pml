g <- ggplot(pdata, aes(x = smean));
g <- g + geom_histogram(aes(y=..density..), colour="black",
fill = "blue")
g + geom_density(colour="red", size=1);
pdata<-data.frame(smean)
g <- ggplot(pdata, aes(x = smean));
g <- g + geom_histogram(aes(y=..density..), colour="black",
fill = "blue")
g + geom_density(colour="red", size=1)
g<-g+labs(title="Density of 40 Exponentials", x="Mean of 40 Exponentials", y="Density")
g<-g+vline(xintercept=pmean, size=1.0, color="red", )
power.t.test(power=.8, n=27, sd=1, type="one.sample". alt="one.sided")$delta
power.t.test(power=.8, n=27, sd=1, type="one.sample", alt="one.sided")$delta
round(pbinom(2,size=4,prob=0.5,lower.tail=FALSE),2)
pbinom (3,size=4,prob = 0.5)
binom.test(3,4,p=0.5,alternative="g")
poisson.test(10,T=1787,r=1/100,alternative="l")
pooled.var = (1.5^2*8 + 1.8^2*8)/(9+9-2)
test_stat <- -3-1/sqrt(pooled.var/9)
pt(test_stat,8)
pt(test_stat,8) < .01
n1 <- n2 <- 9
x1 <- -3  ##treated
x2 <- 1  ##placebo
s1 <- 1.5  ##treated
s2 <- 1.8  ##placebo
spsq <- ( (n1 - 1) * s1^2 + (n2 - 1) * s2^2) / (n1 + n2 - 2)
t=(x1-x2)/(spsq*sqrt(1/n1 + 1/n2))
2*pt(t,n1+n2-2)
90% C.I. [1077,1023]
symmetric to 1000
sample size n=9
t distribution
power.t.test(power=0.9, delta=0.01, sd=0.04, sig.level=0.05, type="one.sample", alt="one.sided")$n
library(UsingR)
data(father.son)
x<-father.son$sheight
n<-length(x)
B<-10000
resamples<-matrix(sample(x, n*B, replace=TRUE), B, n)
install.packages("UsingR")
library(UsingR)
data(father.son)
x<-father.son$sheight
n<-length(x)
B<-10000
resamples<-matrix(sample(x, n*B, replace=TRUE), B, n)
str(resamples)
View(resamples)
x <- c(0.18, -1.54, 0.42, 0.95)
w <- c(2, 1, 3, 1)
y<-rep(w, mean(x))
y
?rep
y<-rep(x, w)
mean(y)
?lm
x <- c(0.8, 0.47, 0.51, 0.73, 0.36, 0.58, 0.57, 0.85, 0.44, 0.42)
y <- c(1.39, 0.72, 1.55, 0.48, 1.19, -1.59, 1.23, -0.65, 1.49, 0.05)
lm(y~x)
lm(y~x-1)
library(datasets)
data(mtcars)
lm(mpg~w)
str(mtcars)
lm(mpg~wt)
lm(mpg~wt, mtcars)
.5*/.5
.5/.5
.4*1.5
x <- c(8.58, 10.46, 9.01, 9.64, 8.86)
(x-mean(x))/(sd(x))
x <- c(0.8, 0.47, 0.51, 0.73, 0.36, 0.58, 0.57, 0.85, 0.44, 0.42)
y <- c(1.39, 0.72, 1.55, 0.48, 1.19, -1.59, 1.23, -0.65, 1.49, 0.05)
lm(y~x)
x <- c(0.8, 0.47, 0.51, 0.73, 0.36, 0.58, 0.57, 0.85, 0.44, 0.42)
mean(x)
swirl()
library(swirl)
swirl()
lhs-rhs
all.equal(lhs, rhs)
exit
quit
varChild <- var(galton$child)
varRes<-var(fit$residuals)
varEst(est(ols.ic+ols.slope*galton$parent))
varEst<-est(ols.ic+ols.slope*galton$parent)
varEst<-est(ols.ic, ols.slope*galton$parent)
varEst <- var(est(ols.slope, ols.ic))
all.equal(varChild, varRes+varEst)
efit <- lm(accel ~ mag+dist, attenu)
mean(efit$residuals)
cov(efit$residuals, attenu$mag)
cov(efit$residuals, attenu$dist)
cor(gpa_nor, gch_nor)
l_nor<-lm(gch_nor ~ gpa_nor)
fit<-lm(child ~ parent, galton)
sum(fit$residuals^2)/(928-2)
sqrt(sum(fit$residuals^2)/(n-2))
summary(fit)$sigma
sqrt(deviance(fit)/(n-2))
mu<-mean(galton$child)
sTot<-sum(galton$child-mu)^2
sTot<-sum((galton$child-mu)^2)
sRe<-deviance(galton$child-)
sRe<-deviance(galton$child-mu)
sRe<-deviance(galton$child, mu)
sRe<-deviance(galton$child)
sRe<-deviance(sTot)
sRe<-sTot
sRes<-deviance(fit)
1-sRes/sTot
all.equal(1-sRes/sTot, summary(fit)$r)
summary(fit)$r.squared
(cor(galton$child, galton$parent))^2
cor(galton$parent,galton$child)^2
swirl
swirl()
x <- c(0.61, 0.93, 0.83, 0.35, 0.54, 0.16, 0.91, 0.62, 0.62)
y <- c(0.67, 0.84, 0.6, 0.18, 0.85, 0.47, 1.1, 0.65, 0.36)
fit<-lm(y ~ x)
fit$p
summary(fit)
fit$residuals
sd(fit$residuals)
sd(fit$residuals)/(9-2)
fit1<-lm(mpg ~ wt, mtcars)
summary(fit1)
sqrt(var(fit$residuals))
sqrt(sum(fit$residuals/(9-1)))
sqrt(sum(fit$residuals/(9-2)))
sqrt(sum(fit$residuals^2/(9-1)))
sumCoef<-summary(fit)$coefficients
sumCoef[2,1]+c(-1, 1)*qt(.975, df=fit$df)*sumCoef[2,2]
b1<-sumCoef[2,1]-1*qt(.975, df=fit$df)*sumCoef[2,2]
b0<-sumCoef[1,1]-1*qt(.975, df=fit$df)*sumCoef[1,2]
b0+b1*mean(wt)
b0+b1*mean(mtcars$wt)
?mtcars
dd<-summary(fit)
View(dd)
str(dd)
sum(fit$residuals)
x <- c(0.61, 0.93, 0.83, 0.35, 0.54, 0.16, 0.91, 0.62, 0.62)
y <- c(0.67, 0.84, 0.6, 0.18, 0.85, 0.47, 1.1, 0.65, 0.36)
fit<-lm(y ~ x)
summary(fit)$coefficients[2,4]
summary(fit)$sigma
data(mtcars)
fit <- lm(mpg ~ wt, data=mtcars)
(summary(fit)$coeff)
newdata <- data.frame(wt=mean(mtcars$wt))
predict(fit, newdata, interval=("confidence"))
newdata <- data.frame(wt=3000/1000)
predict(fit, newdata, interval=("prediction"))
mtcars$wt_ton <- mtcars$wt / 2
fit <- lm(mpg ~ wt_ton, data=mtcars)
(coef <- summary(fit)$coeff)
coef[2,1] + c(-1, 1) * qt(.975, df=fit$df) * coef[2,2]
fit_num <- lm(mpg ~ wt, data=mtcars)
fit_denom <- lm(mpg ~ 1 + offset(0 * wt), data=mtcars)
sum(resid(fit_num)^2) / sum(resid(fit_denom)^2)
summary(mtcars)
head(mtcars)
mtcars2<-mtcars[1, 4, 6]
pairs(mtcars2)
mtcars2<-mtcars[, c(1, 4, 6)]
pairs(mtcars2)
pairs(mtcars2, panel=panel.smooth, main="Pairs of mpg, hp, and wt")
fit1<-lm(mpg ~ hp, mtcars)
fit2<-lm(mpg ~ hp+hp*hp, mtcars)
fit2
summary(fit2)
summary(fit1)
fit2<-lm(mpg ~ hp+wt, mtcars)
summary(fit2)
fit3<-lm(mpg ~ hp+hp*wt, mtcars)
summary(fit3)
pairs(mtcars2, panel=panel.smooth, main="Figure 1. Pair Plots for mpg, hp, and wt")
str(mtcars)
#mtcars$cyl  <- factor(mtcars$cyl)
install.packages("testthat")
?influence.measures
n<-100; x<-c(10, rnorm(n)); y<-c(10, c(rnorm(n)))
plot(x, y, frame=FALSE, cex=2, pch=21, bg="lightblue", col="black")
abline(lm(y~x))
?mtcars
library(MASS)
data(shuttle)
View(shuttle)
shuttle2<-shuttle
shuttle2$use2<-as.numeric(shuttle2$use=='auto')
View(shuttle2)
shuttle2$wind2<-as.numeric(shuttle2$wind=='head')
fit
exp(-10.69)
fit<-glm(use2 ~ factor(wind) - 1, family = binomial, data = shuttle2)
fit
exp(coef(fit))
1.285714/1.327
fit<-glm(use2 ~ factor(wind) , family = binomial, data = shuttle2)
fit
exp(coef(fit))
fit1<-glm(use2 ~ factor(wind) +factor(magn) , family = binomial, data = shuttle2)
fit1
exp(coef(fit1))
fit1<-glm(use2 ~ factor(wind) -1 +factor(magn) , family = binomial, data = shuttle2)
fit1
exp(coef(fit1))
1.4383682/1.4851533
1.438/1.033
library(png);library(grid)
library(doMC); registerDoMC(cores = 4)
grid.raster(readPNG("figures/1.png"))
install.packages("png")
library(png);library(grid)
library(doMC); registerDoMC(cores = 4)
install.packages("doMC")
library(doMC); registerDoMC(cores = 4)
install.packages("doMC")
library(kernlab); data(spam); set.seed(333)
install.packages("kernlab")
library(kernlab); data(spam); set.seed(333)
smallSpam <- spam[sample(dim(spam)[1],size=10),]
View(spam)
View(smallSpam)
spamLabel <- (smallSpam$type=="spam")*1 + 1
plot(smallSpam$capitalAve,col=spamLabel)
rule1 <- function(x){
prediction <- rep(NA,length(x))
prediction[x > 2.7] <- "spam"
prediction[x < 2.40] <- "nonspam"
prediction[(x >= 2.40 & x <= 2.45)] <- "spam"
prediction[(x > 2.45 & x <= 2.70)] <- "nonspam"
return(prediction)
}
table(rule1(smallSpam$capitalAve),smallSpam$type)
rule2 <- function(x){
prediction <- rep(NA,length(x))
prediction[x > 2.8] <- "spam"
prediction[x <= 2.8] <- "nonspam"
return(prediction)
}
table(rule2(smallSpam$capitalAve),smallSpam$type)
table(rule1(spam$capitalAve),spam$type)
# tabulate out of sample error for algorithm 2
table(rule2(spam$capitalAve),spam$type)
rbind("Rule 1" = c(Accuracy = mean(rule1(spam$capitalAve)==spam$type),
"Total Correct" = sum(rule1(spam$capitalAve)==spam$type)),
"Rule 2" = c(Accuracy = mean(rule2(spam$capitalAve)==spam$type),
"Total Correct" = sum(rule2(spam$capitalAve)==spam$type)))
library(caret); library(kernlab); data(spam)
inTrain<-createDataPartition(y=spam$type, p=0.75, list=FALSE)
training<-spam[inTrain,]
testing<-spam[-inTrain,]
modelFit<-train(type~., data=training, method="glm")
install.packages("e1071")
modelFit<-train(type~., data=training, method="glm")
warnings()
modelFit
install.packages("ISLR")
data(Wage)
library(ISLR)
data(Wage)
summary(Wage)
inTrain<-createDataPartition(y=Wage$wage, p=0.7, list=FALSE)
training<-spam[inTrain,]
testing<-spam[-inTrain,]
dim(training); dim(testing)
inTrain<-createDataPartition(y=Wage$wage, p=0.7, list=FALSE)
training<-Wage[inTrain,]
testing<-Wage[-inTrain,]
dim(training); dim(testing)
featurePlot(x=training[, c("age", "education", "jobclass")], y=training$wage, plot"pairs")
featurePlot(x=training[, c("age", "education", "jobclass")],
y=training$wage,
plot"pairs")
featurePlot(x=training[, c("age", "jobclass")],
y=training$wage,
plot"pairs")
featurePlot(x=training[, c("age", "education", "jobclass")],
y=training$wage,
plot="pairs")
View(training)
install.packages("Hmisc")
training<-spam[inTrain,]
library(Hmisc)
cutWage<-cut2(training$wage, g=3)
table(cutWage)
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(1000)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
View(mixtures)
qplot(SuperPlasticizer, data=mixtures, geom="histogram")
qplot(SuperPlasticizer, data=mixtures, geom="histogram")
qplot(SuperPlasticizer, data=mixtures, geom="histogram")
qplot(SuperPlasticizer, data=training, geom="histogram")
qplot(Superplasticize, data=mixtures, geom="histogram")
qplot(mixtures$Superplasticize, geom="histogram")
qplot(log(mixtures$Superplasticize), geom="histogram")
qplot(log(mixtures$Superplasticize+1), geom="histogram")
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
training<-training[, grep("^IL$", names(training))]
View(training)
training<-training[, grep("^IL", colnames(training))]
View(training)
training = adData[ inTrain,]
training<-training[, grep("^IL", colnames(training))]
View(training)
set.seed(3433)
IL <- grep("^IL", colnames(training), value=TRUE)
ILpredictors <- predictors[, IL]
df <- data.frame(diagnosis, ILpredictors)
inTrain <- createDataPartition(df$diagnosis, p=3/4)[[1]]
training <- df[inTrain, ]
testing <- df[-inTrain, ]
modelFit <- train(diagnosis ~ ., method="glm", data=training)
predictions <- predict(modelFit, newdata=testing)
C1 <- confusionMatrix(predictions, testing$diagnosis)
print(C1)
acc1 <- C1$overall[1]
acc1
modelFit <- train(training$diagnosis ~ .,
method="glm",
preProcess="pca",
data=training,
trControl=trainControl(preProcOptions=list(thresh=0.8)))
C2 <- confusionMatrix(testing$diagnosis, predict(modelFit, testing))
print(C2)
acc2 <- C2$overall[1]
acc2
library(caret)
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
vowel.train$y<-factor(vowel.train$y)
vowel.test$y<-factor(vowel.test$y)
set.seed(33833)
mod1<train(y~., data=vowel.train, method="rf", trCountrol=trainControl(method="cv", number=3))
mod1<train(y~., data=vowel.train, method="gbm")
set.seed(33833)
mod1<-train(y~., data=vowel.train, method="rf", trCountrol=trainControl(method="cv", number=3))
mod2<-train(y~., data=vowel.train, method="gbm")
pred1<-predict(mode1, vowel.test); pred2<-predict(mod2, vowel.test)
qplot(pred1, pred2, colour=y, data=vowel.test)
pred1<-predict(mod1, vowel.test); pred2<-predict(mod2, vowel.test)
qplot(pred1, pred2, colour=y, data=vowel.test)
predDF<-data.frame(pred1, pred2, y=vowel.test$y)
combModFit<-train(y~., method="gam", data=predDF)
ComPred<-predict(combModFit, predDF)
c1 <- confusionMatrix(pred1, vowel.test$y)
c2 <- confusionMatrix(pred2, vowel.test$y)
c3 <- confusionMatrix(combModFit, predDF$y)
c3 <- confusionMatrix(combModFit, vowel.test$y)
c1
c2
rf_accuracy<-sum(pred1==vowel.test$y)/length(pred1)
gbm_accuracy<-sum(pred2==vowel.test$y)/length(pred2)
rf_accuracy
gbm_accuracy
agreeData<-vowel.test[pred1==pred2,]
predDF<-data.frame(pred1, pred2, y=vowel.test$y,
agree=pred1==pred2)
accuracy<-sum(pred1[predDF$agree]==predDF$y[predDF$agree])/sum(predDF$agree)
accuracy
library(caret)
library(gbm)
set.seed(3433)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
set.seed(62433)
mod1<-train(diagnosis~., data=training, method="rf", trCountrol=trainControl(method="cv", number=3))
mod2<-train(diagnosis~., data=training, method="gbm")
mod3<-train(diagnosis~., data=training, method="lda")
pred1<-predict(mod1, testing)
pred2<-predict(mod2, testing)
pred2<-predict(mod2, testing)
confusionMatrix(pred1, testing$diagnosis)$overall[1]
confusionMatrix(pred2, testing$diagnosis)$overall[1]
confusionMatrix(pred3, testing$diagnosis)$overall[1]
predData<-data.frame(pred1, pred2, pred3, diagnosis=testing$diagnosis)
comb_mod<-train(diagnosis~., data=predData, method="rf",
trCountrol=trainControl(method="cv", number=3))
pred_comb<-predict(comb_mode, predData)
confusionMatrix(pred_comb, testing$diagnosis)$overall[1]
pred2<-predict(mod2, testing)
pred3<-predict(mod2, testing)
confusionMatrix(pred1, testing$diagnosis)$overall[1]
confusionMatrix(pred2, testing$diagnosis)$overall[1]
confusionMatrix(pred3, testing$diagnosis)$overall[1]
pred1<-predict(mod1, testing)
pred2<-predict(mod2, testing)
pred3<-predict(mod3, testing)
confusionMatrix(pred1, testing$diagnosis)$overall[1]
confusionMatrix(pred2, testing$diagnosis)$overall[1]
confusionMatrix(pred3, testing$diagnosis)$overall[1]
predData<-data.frame(pred1, pred2, pred3, diagnosis=testing$diagnosis)
comb_mod<-train(diagnosis~., data=predData, method="rf",
trCountrol=trainControl(method="cv", number=3))
pred_comb<-predict(comb_mode, predData)
pred_comb<-predict(comb_mod, predData)
confusionMatrix(pred_comb, testing$diagnosis)$overall[1]
set.seed(62433)
mod1<-train(diagnosis~., data=training, method="rf")
mod2<-train(diagnosis~., data=training, method="gbm")
mod3<-train(diagnosis~., data=training, method="lda")
pred1<-predict(mod1, testing)
pred2<-predict(mod2, testing)
pred3<-predict(mod3, testing)
predData<-data.frame(pred1, pred2, pred3, diagnosis=testing$diagnosis)
comb_mod<-train(diagnosis~., data=predData, method="rf")
pred_comb<-predict(comb_mod, testing)
c1<-confusionMatrix(pred1, testing$diagnosis)$overall[1]
c2<-confusionMatrix(pred2, testing$diagnosis)$overall[1]
c3<-confusionMatrix(pred3, testing$diagnosis)$overall[1]
c4<-confusionMatrix(pred_comb, testing$diagnosis)$overall[1]
print(paste(c1, c2, c3, c4))
mod2<-train(diagnosis~., data=training, method="gbm", verbose=FALSE)
library(caret)
library(gbm)
set.seed(3433)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
set.seed(62433)
mod1<-train(diagnosis~., data=training, method="rf")
mod2<-train(diagnosis~., data=training, method="gbm", verbose=FALSE)
mod3<-train(diagnosis~., data=training, method="lda")
pred1<-predict(mod1, testing)
pred2<-predict(mod2, testing)
pred3<-predict(mod3, testing)
predData<-data.frame(pred1, pred2, pred3, diagnosis=testing$diagnosis)
comb_mod<-train(diagnosis~., data=predData, method="rf")
pred_comb<-predict(comb_mod, testing)
c1<-confusionMatrix(pred1, testing$diagnosis)$overall[1]
c2<-confusionMatrix(pred2, testing$diagnosis)$overall[1]
c3<-confusionMatrix(pred3, testing$diagnosis)$overall[1]
c4<-confusionMatrix(pred_comb, testing$diagnosis)$overall[1]
print(paste(c1, c2, c3, c4))
set.seed(3523)
library(AppliedPredictiveModeling)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]
names(training)
set.seed(233)
mod1<-train(CompressiveStrength~., data=training, method="lasso")
?plot.enet
plot.enet(fit$finalModel, xvar="penalty", use.color=T)
plot.enet(mod1$finalModel, xvar="penalty", use.color=T)
library(lubridate)  # For year() function below
dat = read.csv("https://d396qusza40orc.cloudfront.net/predmachlearn/gaData.csv")
training = dat[year(dat$date) < 2012,]
testing = dat[(year(dat$date)) > 2011,]
tstrain = ts(training$visitsTumblr)
install.packages("lubridate")
library(lubridate)  # For year() function below
dat = read.csv("https://d396qusza40orc.cloudfront.net/predmachlearn/gaData.csv")
training = dat[year(dat$date) < 2012,]
testing = dat[(year(dat$date)) > 2011,]
tstrain = ts(training$visitsTumblr)
library(lubridate)  # For year() function below
dat = read.csv("https://d396qusza40orc.cloudfront.net/predmachlearn/gaData.csv")
training = dat[year(dat$date) < 2012,]
testing = dat[(year(dat$date)) > 2011,]
tstrain = ts(training$visitsTumblr)
fit <- bats(tstrain)
install.packages("forecast")
fit <- bats(tstrain)
?bats
??bats
library(forecast)
fit <- bats(tstrain)
fit
pred <- forecast(fit, level=95, h=dim(testing)[1])
names(data.frame(pred))
predComb <- cbind(testing, data.frame(pred))
names(testing)
names(predComb)
predComb$in95 <- (predComb$Lo.95 < predComb$visitsTumblr) &
(predComb$visitsTumblr < predComb$Hi.95)
# How many of the testing points is the true value within the
# 95% prediction interval bounds?
prop.table(table(predComb$in95))[2] # 0.9617021
?1071
?E1071
?e1071
??e1071
setwd("~/R/Coursera/pml")
training <- read.csv(file="https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv",head=TRUE,sep=",")
test<-read.csv(file="https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv",head=TRUE,sep=",")
str(training)
